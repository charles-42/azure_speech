{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AZURE SPEECH SDK PRESENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on commence par installer le SDK: pip install azure-cognitiveservices-speech\n",
    "\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from config import key, location\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La [documentation/tutp](https://docs.microsoft.com/fr-fr/azure/cognitive-services/speech-service/)\n",
    "\n",
    "La [documentation/détail des objets](https://docs.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/azure.cognitiveservices.speech?view=azure-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Speech to Text (via le SDK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe trois moyen d'utiliser speech to text:\n",
    "- via le [CLI](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/spx-overview) (no code, simple usage)\n",
    "- via le [SDK](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-sdk?tabs=windows%2Cubuntu%2Cios-xcode%2Cmac-xcode%2Candroid-studio) (à privilégier)\n",
    "- via les [API REST](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/rest-speech-to-text)\n",
    "  - Speech-to-text REST API v3.0 is used for Batch transcription and Custom Speech.\n",
    "  - Speech-to-text REST API for short audio is used for online transcription (comme transcription continue mais limité à 60 seconde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les objets du SDK speech-to-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le Kit de développement logiciel (SDK) possède 4 objets principaux:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![speech to text](https://docs.microsoft.com/en-us/learn/wwl-data-ai/transcribe-speech-input-text/media/speech-to-text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SpeechConfig pour encapsuler les informations de connexion à votre ressource Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=key, region=location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. AudioConfig pour définir la source d’entrée de l’audio à transcrire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format should be .wav or .ogg\n",
    "audio_input = speechsdk.AudioConfig(filename=\"absinthe.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. SpeechRecognizer: le client proxy pour l’API de reconnaissance vocale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's here you change the recognition source.\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config,language=\"fr-FR\", audio_config=audio_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Microphone`**\n",
    "\n",
    "Si dans le speech_recognizer aucun audioConfig n'est précisée alors par défaut la source audio est votre microphone(si vous avez plusieurs microphone vous pouvez [indiquer dans un audioconfig](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/get-started-speech-to-text?tabs=windowsinstall&pivots=programming-language-python) l'id de votre microphone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Le dernier objet important est le retour obtenu mais il dépend de la méthode appelée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les méthodes de speechrecognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recognize_once() et recognize_once_async() : Reconnaitre un mot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns after a single utterance is recognized. The end of a single utterance is determined by listening for silence at the end or until a maximum of 15 seconds of audio is processed.\n",
    "\n",
    "Comme toujours, la méthode avec async permet de ne pas arreter l'execution code si l'opération prend du temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = speech_recognizer.recognize_once_async().get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le retour est un [SpeechRecognitionResult](https://docs.microsoft.com/en-us/javascript/api/microsoft-cognitiveservices-speech-sdk/speechrecognitionresult?view=azure-node-latest)\n",
    "Il possède les attributs suivants:\n",
    "- reason\n",
    "  - RecognizedSpeech: la reconnaissance a fonctionné\n",
    "  - NoMatch: la reconnaissance a fonctionné mais rien n'a pu être reconnu\n",
    "  - Canceled: erreur ou annulation\n",
    "    - il y un object: result.cancellation_details.raison ..\n",
    "- Duration: Duration of recognized speech\n",
    "- Text: résultat de la reconnaissance.\n",
    "- language: Primary Language detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file_name.txt', 'w') as f:\n",
    "    f.write(result.text)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start_continuous_recognition() reconnaitre un texte "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La reconnaissance continue nécessite 4 étapes:\n",
    "- le set up \n",
    "- le lancement de la reconnaissance\n",
    "- le traitement des événements\n",
    "- l'arret de la reconnaissance;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de commencer il faut comprendre une chose:\n",
    "Quand une session de reconnaissance continue est lancée, cette opération prend du temps. Au cours de celle-ci des événements sont envoyés à l'EventSignal. Il est possible de se connecter à cet event Signal pour recevoir tous les signals correspondant.\n",
    "\n",
    "L'objet speech_recognizer possède plusieurs attributs dont il sera possible d'obtenir les événements:\n",
    "- `recognizing`: Signal for events containing intermediate recognition results.\n",
    "- `recognized`: Signal for events containing final recognition results (indicating a successful recognition attempt).\n",
    "- `session_started`: Signal for events indicating the start of a recognition session (operation).\n",
    "- `session_stopped`: Signal for events indicating the end of a recognition session (operation).\n",
    "- `canceled`: Signal for events containing canceled recognition results (indicating a recognition attempt that was canceled as a result or a direct cancellation request or, alternatively, a transport or protocol failure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple pour l'événement recognized. On peut créer un client pour détecter ces événements à l'aide du code ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer.recognized.connect(\"function name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque événement détecter on lui fait passer une fonction qui peut avoir plusieurs but:\n",
    "- afficher l'événement\n",
    "- enregistrer la sortie de l'événement(le texte reconnu)\n",
    "- arreter la reconnaissance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut à présent définir le setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on définit les fonctions qui vont être utiles\n",
    "# une pour arreter la reconnaissance\n",
    "def stop_cb(evt):\n",
    "    \"\"\"callback that stops continuous recognition upon receiving an event `evt`\"\"\"\n",
    "    print('CLOSING on {}'.format(evt))\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "# une pour enregistrer le résultat\n",
    "all_results = []\n",
    "def handle_final_result(evt):\n",
    "    all_results.append(evt.result.text)\n",
    "\n",
    "# on initilise ensuite nos clients et on leur fait passer les fonctions voulues\n",
    "# le client lié au début de session\n",
    "speech_recognizer.session_started.connect(lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "\n",
    "# les clients liées à la reconnaissance du texte\n",
    "speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))\n",
    "\n",
    "speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt)))\n",
    "speech_recognizer.recognized.connect(handle_final_result)\n",
    "\n",
    "# les clients liées à la fin de session\n",
    "speech_recognizer.session_stopped.connect(lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "speech_recognizer.session_stopped.connect(stop_cb)\n",
    "\n",
    "speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
    "speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "# On peut ensuite lancer la reconnaissance.\n",
    "speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "#affiche le résultat\n",
    "#print(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options supplémentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mode will cause the speech config instance to interpret word descriptions of sentence structures such as punctuation. \n",
    "\n",
    "For example, the utterance \"Do you live in town question mark\" would be interpreted as the text \"Do you live in town?\".\n",
    "\n",
    "To enable dictation mode, use the enable_dictation() method on your SpeechConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpeechConfig.enable_dictation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phrase list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut donner une liste de phrase qu'on sait être dans le texte pour aider la reconnaissance vocale.\n",
    "\n",
    "`Par exemple` si on sait quand dans le texte on a \"Move to Ward\" on peut ajouter cette phrase à la liste pour éviter que azure comprenne \"Move toward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on crée  pour cela un objet phraselistgrammar\n",
    "phrase_list_grammar = speechsdk.PhraseListGrammar.from_recognizer(speech_recognizer)\n",
    "phrase_list_grammar.addPhrase(\"Move to Ward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'objet possède également une méthode clear\n",
    "phrase_list_grammar.clear()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64f75c8df398d419dce6ed30da344261df61aeab196b7e1f2bdb0075de53f469"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
